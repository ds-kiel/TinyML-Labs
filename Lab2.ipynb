{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ds-kiel/TinyML-Labs/blob/WS23-24/Lab2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ds-kiel/TinyML-Labs/blob/WS23-24/Lab2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://raw.githubusercontent.com/ds-kiel/TinyML-Labs/WS23-24/Lab2.ipynb\" download><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "MzHYKcWEHTGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'Group\\<Your group number\\>_Lab1.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UdQ1KVS0HZXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "THIS IS THE FINAL VERSION OF THE LAB!\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-wxE3-Ze5Mq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Quantization and On-Device Execution\n",
        "\n",
        "In the first lab you looked at the first part of the pipeline from data to executing models on low-power devices. You explored how to train neural networks with Tensorflow and how to profile a model's memory usage for a specific micrcontroller. In this lab we continue the pipeline and you will explore how to [convert](https://www.tensorflow.org/lite/models/convert/convert_models) a model to a [Tensorfow Lite (TFLite)](https://www.tensorflow.org/lite) model, how to [quantize](https://www.tensorflow.org/lite/performance/post_training_integer_quant) [a model](https://www.tensorflow.org/model_optimization/guide/quantization/post_training), how to use [quantization-aware training](https://www.tensorflow.org/model_optimization/guide/quantization/training) and finally how to deploy the model and use the model with a microcontroller.\n",
        "\n",
        "Like in the last lab, you once explore the full pipeline using a model trained on the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist). Afterward, you use your trained models of the second part of lab 1 and convert, deploy, and execute them on a microcontroller, specifically the [Arduino Nano 33 BLE Sense](https://store.arduino.cc/products/arduino-tiny-machine-learning-kit).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OvrMU-DIHePo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "p2d8yFY1GUX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have not done so already, install the following dependencies\n",
        "!python -m pip install tensorflow tensorflow-model-optimization scikit-learn edgeimpulse numpy matplotlib seaborn"
      ],
      "metadata": {
        "id": "fOJvKcsUGWH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "eTICd15rGZGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# from tensorflow.lite import TFLiteConverter\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import edgeimpulse as ei\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "y7yRUuJUGaq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "yhIKkymAGfqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle(f'Model {model_name}')\n",
        "    fig.set_figwidth(15)\n",
        "\n",
        "    ax1.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'])\n",
        "    ax1.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'])\n",
        "    ax1.set_title('Model accuracy')\n",
        "    ax1.set(xlabel='epoch', ylabel='accuracy')\n",
        "    ax1.legend(['training', 'validation'], loc='best')\n",
        "\n",
        "    ax2.plot(range(1, len(history.history['loss'])+1), history.history['loss'])\n",
        "    ax2.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'])\n",
        "    ax2.set_title('Model loss')\n",
        "    ax2.set(xlabel='epoch', ylabel='loss')\n",
        "    ax2.legend(['training', 'validation'], loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WRxTHnJcGk0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edge Impulse API Key\n",
        "\n",
        "Insert your Edge Impulse API Key as in Lab 1:"
      ],
      "metadata": {
        "id": "62RGknKGGopG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ei.API_KEY = \"ei_dae2...\" # Change this to your Edge Impulse API key"
      ],
      "metadata": {
        "id": "ERLWd1g2GzXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "mp9j7urCG-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the data"
      ],
      "metadata": {
        "id": "jZUVTQ2TIA10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model / data parameters\n",
        "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "num_classes = len(labels)\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "49iKC6oXIDDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the model\n",
        "\n",
        "---\n",
        "**Task 1:** Add and train your best MNIST model from lab 1, that has a small enough memory footprint to fit onto the target platform.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yt8W5N0UIHtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build MNIST model\n",
        "def build_model_mnist(summary=False):\n",
        "    model = Sequential()\n",
        "\n",
        "    # ADD YOUR LAYERS HERE\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile model_mnist\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    if summary:\n",
        "        model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "eWq43Tx6HxN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mnist = build_model_mnist()"
      ],
      "metadata": {
        "id": "sSs127krIdEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n"
      ],
      "metadata": {
        "id": "t1n_O2a6I0ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_cb = EarlyStopping(\n",
        "    monitor=...,\n",
        "    patience=...,\n",
        "    min_delta=...,\n",
        "    mode=...\n",
        ")\n",
        "\n",
        "num_epochs = 200\n",
        "history_mnist = model_mnist.fit(x_train, y_train, batch_size=128, epochs=num_epochs, validation_split=0.1, callbacks=[early_stopping_cb])\n",
        "plot_training_history(history_mnist, 1)"
      ],
      "metadata": {
        "id": "aCrSiGBcI2ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model\n",
        "\n",
        "In the previous lab, you always retrained a model when you continued working on the tasks. However, to come back to a model it might be useful to save it. We can use the `model.save()` [Function](https://www.tensorflow.org/guide/keras/serialization_and_saving) that exports a TensorFlow model object to SavedModel format.\n",
        "\n",
        "If you use Google Colab, you can find the saved model as a `.keras`-file on the left under `Files/`."
      ],
      "metadata": {
        "id": "2XntBfQV0sz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jc29JrFHMMW"
      },
      "outputs": [],
      "source": [
        "export_path = 'model_mnist.keras'\n",
        "model_mnist.save(export_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Task 2:** Profile the memory usage of your model with Edge Impulse.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oSPnHmpNK0g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD YOUR MEMORY ESTIMATION HERE!"
      ],
      "metadata": {
        "id": "bMucH2IMLC4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Quantization\n",
        "\n",
        "Your microcontroller cannot use the Tensoflow model directly. Instead there is [Tensorflow Lite](https://www.tensorflow.org/lite) for deploying models on mobile and edge devices.\n",
        "\n",
        "---\n",
        "**Task 3:** Load your MNIST model and convert it with Tensorflow Lite and save the model to a `.tflite`-file. (HINT: Check out [this](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/hello_world) *Hello World* example.)\n",
        "\n",
        "**Task 4:** Create a second Tensorflow Lite conversion that uses [optimizations](https://www.tensorflow.org/lite/api_docs/python/tf/lite/Optimize) and enforce integer-only weights.\n",
        "(Maybe a helpful [resource](https://www.tensorflow.org/lite/performance/post_training_quantization).)\n",
        "\n",
        "**Task 5:** Create a third Tensorflow Lite conversion that in addition to the conversion in *Task 4* enforces integer-only quantization. (*Hint: Use a [representative dataset](https://www.tensorflow.org/lite/api_docs/python/tf/lite/RepresentativeDataset).*)\n",
        "\n",
        "**Task 6:** Evaluate all three converted models and compare them to the Tensorflow model they are based on regarding profiled memory usage and accuracy. Use plots.\n",
        "\n",
        "**Task 7:** Explain your findings from Task 6. Why is there such a difference in performance and in memory usage?\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OCiGJ8HJLuyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADD YOUR MODEL CONVERSIONS HERE"
      ],
      "metadata": {
        "id": "7lG5IDfbLQT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the converted model\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "ix1KJnp22zz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization Aware Training\n",
        "\n",
        "To improve on the performance of your converted models, you can use Quantization Aware Training before converting the model.\n",
        "\n",
        "---\n",
        "**Task 8:** Run the code below and explain the resulting model.\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "01fm7CJQLthE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('model_mnist.keras')\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for for quantization aware.\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "sQMcvQodmSzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Task 9:** Train (fit) the model and save it for future use. Make sensible choices for the number of epochs. Show the training performance.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CAmGK5OcodGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN THE MODEL HERE"
      ],
      "metadata": {
        "id": "UC9yCeX5pB6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Task 10:** Quantize this model. And save it.\n",
        "\n",
        "**Task 11:** Evaluate the performance of the model after quantization-aware training and after additional quantization. Set it into perspective to the original model and the best quantized version of the original model. Compare memory usage and accuracy. Use plots. (You should compare 4 models here.)\n",
        "\n",
        "**Task 12:** Explain your findings from *Task 11*.\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gxK9kB02pQUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Export - Library Creation\n",
        "\n",
        "Up until now we created different models that we can test and evaluate using Python. However, most microcontrollers don't speak Python. Instead they work with C/C++ and thus we need a C(++) library of the models to execute it. Here you explore different ways to export your models to a C(++) library."
      ],
      "metadata": {
        "id": "j44g7Vu3qFY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manual conversion of the model\n",
        "\n",
        "---\n",
        "**Task 13:** Convert your best performing quantized model to a C++ library with the code below and explain the content of the two resulting files.\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lfrD5yGMrbOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get -qq install xxd"
      ],
      "metadata": {
        "id": "b4E9P-rGrcY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_TFLITE = 'model.tflite' #enter the name of your TFlite file uploaded to the folders section\n",
        "MODEL_TFLITE_MICRO = 'model.cc' #update the name of your .cc file (This can be anything)\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ],
      "metadata": {
        "id": "SgPYODpY0zFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIBRARY_NAME = 'mnist_model'\n",
        "max_label_str_length = max([len(lbl) for lbl in labels]) + 1\n",
        "\n",
        "model_str = f\"alignas(16) const unsigned char {LIBRARY_NAME}[] = \"\n",
        "with open(MODEL_TFLITE_MICRO, 'r') as file:\n",
        "    data = file.read();\n",
        "    model_str += data[data.index(\"{\"): len(data)].replace(\"unsigned\", \"const\")\n",
        "\n",
        "labels_str = f\"const char available_classes[][{max_label_str_length}] = {{\"\n",
        "for i in range(0, len(labels)):\n",
        "    if i != 0:\n",
        "        labels_str += \", \"\n",
        "    labels_str += \"\\\"\"+labels[i]+\"\\\"\"\n",
        "labels_str += \"};\"\n",
        "\n",
        "output_str = f\"#include \\\"{LIBRARY_NAME}.h\\\"\\n\"\n",
        "output_str += labels_str + \"\\n\"\n",
        "output_str += \"const int available_classes_num = \"+str(len(labels)) +\";\\n\"\n",
        "output_str += model_str\n",
        "\n",
        "with open(f\"{LIBRARY_NAME}.cpp\", \"w\") as file:\n",
        "    file.write(output_str)\n",
        "\n",
        "header_str = \"#ifndef TENSORFLOW_LITE_MODEL_H_\\n#define TENSORFLOW_LITE_MODEL_H_\\n\\n\"\n",
        "header_str += \"// Classes that can be detected by the neural network\\n\"\n",
        "header_str += f\"extern const char available_classes[][{max_label_str_length}];\\n\"\n",
        "header_str += \"extern const int available_classes_num;\\n\\n\"\n",
        "header_str += \"// Pre-trained netural network\\n\"\n",
        "header_str += f\"extern const unsigned char {LIBRARY_NAME}[];\\n\"\n",
        "header_str += f\"extern const int {LIBRARY_NAME}_len;\\n\\n\"\n",
        "header_str += \"#endif /* TENSORFLOW_LITE_MODEL_H_ */\"\n",
        "\n",
        "with open(f\"{LIBRARY_NAME}.h\", \"w\") as file:\n",
        "    file.write(header_str)\n"
      ],
      "metadata": {
        "id": "ykMRIHrGrXBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next you will use your library in an Arduino program to (or if you prefer, in a Zephyr program) and execute the [inference on a microcontroller](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level). I strongly recommend you to use [this](https://docs.arduino.cc/tutorials/nano-33-ble-sense/get-started-with-machine-learning) Arduino example as a starting point to write the code. (If you prefer to use Zephyr, have a look at [this](https://github.com/ds-kiel/blueseer/) repository.)\n",
        "\n",
        "---\n",
        "**Task 14:** Write an Arduino program that takes a representation (e.g., an array of values) of an MNIST image as input (through the serial interface), classifies the image and reports the result back to you through the serial interface. *If you like, write a companion program on that performs the image transformation and serial communication.*\n",
        "\n",
        "**Task 15:** Upload the program to the Arduino and compare the real memory usage with the Edge Impulse estimate. Was the estimate correct? How much does it differ?\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "**Task 16:** Extend your Arduino program and measure the inference time on the Arduino. Was the estimate correct?\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "**Task 17:** Perform inference for at least 20 images (more is better) and plot statistics (e.g., bar plot (mean) with error bar (standard deviation)) for the inference time. Does it vary? Why or why not?\n",
        "\n",
        "**Answer:** ...\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lZSEjoYYlvXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model conversion with Edge Impulse (Optional task)\n",
        "\n",
        "In the last lab, you used the profiling capabilities of Edge Impulse. Now you will explore how to use Edge Impulse to [create a library](https://docs.edgeimpulse.com/docs/tools/edge-impulse-python-sdk) to deploy your model. First, check the available target devices for deployment (`ei.model.list_deployment_targets()`) and find the correct Arduino corresponding to your hardware.\n",
        "\n",
        "---\n",
        "**Task 18 (optional):** Create an Arduino Library with Edge Impulse and answer Tasks 14-17 for this method. Doing this is a good preperation for Lab 3, in which we will dive deeper into Edge Impulse.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zoE3VswXrgT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FashionMNIST or CIFAR10\n",
        "\n",
        "---\n",
        "**Tasks:**\n",
        "- **Task 19:** Use a well performing model from lab 1 and extend it with quantization aware training, quantize it, and convert it to a C++ library.\n",
        "- **Task 20:** Evaluate and compare the performance of all the intermedite models (including the input model and the output model) regarding accuracy, memory consumption and inference time.\n",
        "- **Task 21:** Create an Arduino program that takes an image as serial input and outputs the prediction of the class of the image (e.g., 'truck' if you use CIFAR10).\n",
        "- **Task 22:** Measure the memory consumption on the Arduino.  Does it match the predicted values?\n",
        "- **Task 23:** Measure the inference time on the Arduino (for multiple (>20) images). Does it match the predicted value?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aqAM8QY05WnU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jX3pYq6F7TNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
